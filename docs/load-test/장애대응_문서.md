# 장애 대응 문서

## 1. 장애 시나리오 및 대응 방안

### 1.1 쿠폰 초과 발급 장애

#### 장애 상황
- 쿠폰 정책: 최대 1000개
- 실제 발급: 1000개 초과
- 원인: 동시성 제어 실패

#### 대응 방안
```java
// 1. 비관적 락 강화
@Lock(LockModeType.PESSIMISTIC_WRITE)
@Query("SELECT c FROM CouponPolicy c WHERE c.policyId = :policyId")
CouponPolicy findByIdForUpdate(@Param("policyId") int policyId);

// 2. 원자적 업데이트
@Modifying
@Query("UPDATE CouponPolicy c SET c.issuedCount = c.issuedCount + 1 " +
       "WHERE c.policyId = :policyId AND c.issuedCount < c.maxCount")
int incrementIssuedCount(@Param("policyId") int policyId);
```

#### 복구 절차
1. **즉시 대응**: 쿠폰 발급 중단
2. **데이터 검증**: 발급 수량 확인
3. **데이터 정정**: 초과 발급 쿠폰 회수
4. **시스템 재시작**: 수정된 코드 배포

### 1.2 데이터베이스 연결 장애

#### 장애 상황
- DB 커넥션 풀 고갈
- 연결 타임아웃 발생
- 서비스 응답 지연

#### 대응 방안
```yaml
# 커넥션 풀 설정 최적화
spring:
  datasource:
    hikari:
      maximum-pool-size: 20
      minimum-idle: 5
      connection-timeout: 30000
      idle-timeout: 600000
      max-lifetime: 1800000
      leak-detection-threshold: 60000
```

#### 복구 절차
1. **모니터링**: 커넥션 풀 상태 확인
2. **확장**: 풀 크기 증가
3. **재시작**: 애플리케이션 재시작
4. **검증**: 서비스 정상화 확인

### 1.3 메모리 부족 장애

#### 장애 상황
- JVM 힙 메모리 부족
- OutOfMemoryError 발생
- GC 압박으로 인한 성능 저하

#### 대응 방안
```bash
# JVM 옵션 최적화
java -Xms1g -Xmx2g \
     -XX:+UseG1GC \
     -XX:MaxGCPauseMillis=200 \
     -XX:+HeapDumpOnOutOfMemoryError \
     -XX:HeapDumpPath=/tmp/heapdump.hprof
```

#### 복구 절차
1. **모니터링**: 메모리 사용량 확인
2. **덤프 분석**: 힙 덤프 파일 분석
3. **코드 최적화**: 메모리 누수 수정
4. **리소스 증설**: 메모리 할당량 증가

## 2. 모니터링 및 알림

### 2.1 핵심 메트릭
- **응답시간**: 95% < 1초
- **에러율**: < 5%
- **처리량**: > 10 req/s
- **DB 커넥션**: 사용률 < 80%

### 2.2 알림 임계값
```yaml
alerts:
  response_time:
    threshold: 1000ms
    severity: warning
  error_rate:
    threshold: 5%
    severity: critical
  db_connections:
    threshold: 80%
    severity: warning
  memory_usage:
    threshold: 85%
    severity: warning
```

### 2.3 대시보드 구성
- **실시간 모니터링**: Grafana
- **로그 분석**: ELK Stack
- **메트릭 수집**: InfluxDB + Prometheus

## 3. 장애 복구 절차

### 3.1 1단계: 장애 감지
1. 모니터링 시스템 알림 수신
2. 장애 유형 및 범위 파악
3. 영향도 평가 (사용자 수, 비즈니스 영향)

### 3.2 2단계: 초기 대응
1. **즉시 조치**: 서비스 중단 또는 제한
2. **팀 소집**: 장애 대응팀 구성
3. **상황 공유**: 관련자에게 상황 전파

### 3.3 3단계: 원인 분석
1. **로그 분석**: 애플리케이션 로그 확인
2. **메트릭 분석**: 성능 지표 검토
3. **시스템 상태**: 인프라 상태 점검

### 3.4 4단계: 복구 작업
1. **임시 조치**: 서비스 복구
2. **근본 원인**: 장애 원인 해결
3. **검증**: 서비스 정상화 확인

### 3.5 5단계: 사후 처리
1. **장애 보고서**: 상세 분석 및 개선안
2. **예방 조치**: 재발 방지 대책
3. **팀 교육**: 학습 및 공유

## 4. 예방 조치

### 4.1 코드 품질
- **단위 테스트**: 커버리지 80% 이상
- **통합 테스트**: 주요 시나리오 검증
- **부하 테스트**: 정기적 성능 검증

### 4.2 인프라 안정성
- **리소스 모니터링**: 실시간 상태 확인
- **자동 스케일링**: 부하에 따른 자동 확장
- **백업 및 복구**: 정기적 백업 및 복구 테스트

### 4.3 운영 프로세스
- **배포 전 검증**: 스테이징 환경 테스트
- **롤백 계획**: 문제 발생 시 즉시 롤백
- **문서화**: 운영 가이드 및 장애 대응 매뉴얼

## 5. 프로젝트 회고 및 개선사항

### 5.1 성공 요인
- **체계적 접근**: 계획 → 실행 → 분석 → 개선의 순환
- **자동화**: k6 기반 부하 테스트 자동화
- **모니터링**: 실시간 성능 지표 추적
- **문서화**: 체계적인 지식 관리

### 5.2 개선이 필요한 부분
- **테스트 커버리지**: 더 다양한 시나리오 필요
- **자동화 수준**: CI/CD 파이프라인 통합 부족
- **모니터링**: 알림 시스템 구축 필요
- **장애 복구**: 자동 복구 메커니즘 부족

### 5.3 다음 단계 계획
- **성능 최적화**: 인덱스 추가, 쿼리 튜닝
- **확장성 개선**: 수평 확장 방안 연구
- **안정성 강화**: 자동 복구 시스템 구축
- **운영 효율성**: 모니터링 및 알림 시스템 고도화
